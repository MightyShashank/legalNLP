{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e222fc",
   "metadata": {},
   "source": [
    "# LegalNLP Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2496fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython # This function gives one access to the current IPython Shell (used in jupyter nb), jupyter nb uses IPython under the hood\n",
    "\n",
    "get_ipython().cache_size = 0 # Generally what output is generated for each cell is stored in RAM, but switching off this doesnt store it (cause later on we would be training in batch sizes of 64 and we'd need RAM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c8f59e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Now the below code snippet is meant to clean the current py workspace\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m \u001b[38;5;66;03m# our garbage collector\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_workspace\u001b[39m():\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaning workspace...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now the below code snippet is meant to clean the current py workspace\n",
    "import gc # our garbage collector\n",
    "import torch\n",
    "\n",
    "def clean_workspace():\n",
    "    print(\"Cleaning workspace...\")\n",
    "\n",
    "    # Delete all global variables in the global scope except system modules\n",
    "    global_vars = list(globals().keys)\n",
    "    for var in global_vars:\n",
    "        if var not in [\"gc\", \"torch\", \"clean_workspace\"]: # Keep required modules and function\n",
    "            del globals()[var]\n",
    "        print(\"Clearing GPU memory\")\n",
    "        torch.cuda.synchronize() # Synchronize all pending GPU operations\n",
    "        torch.cuda.empty_cache() # Clears unused memory from GPU that pyTorch was holding onto \n",
    "        print(\"Running Garbage Collection...\")\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"Workspace Cleaned...\")\n",
    "\n",
    "clean_workspace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers bert_score evaluate\n",
    "!pip install rouge_score tqdm\n",
    "!pip install nltk torch scikit-learn pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5466f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do see 2 T4 GPUs on kaggle but lets verify it\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "import nltk\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import evaluate\n",
    "from bert_score import score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sentence Transformer for embeddings (it embeds a complete sentence rather than per-token wise)\n",
    "sentence_model = SentenceTransformer(\"Stern5497/sbert-legal-xlm-roberta-base\", device=\"cuda\") # We use gpu since it was available\n",
    "# As per my test its length is 768\n",
    "# embedding = model.encode(\"The defendant shall appear before the court\")\n",
    "# print(embedding.shape) = (768,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148b886",
   "metadata": {},
   "source": [
    "# Now lets create/get our train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9841de",
   "metadata": {},
   "source": [
    "We get data from multiple sources and prepare them into one common one for our summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our final dataset\n",
    "data_train = [] # Stores json of judgement + summary\n",
    "data_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f4a6d",
   "metadata": {},
   "source": [
    "## Civil Sum dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d183a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_civilSum_train = pd.read_csv(\"/kaggle/input/civilsum-dataset/CivilSum_train_set.csv\")\n",
    "df_civilSum_test = pd.read_csv(\"/kaggle/input/civilsum-dataset/CivilSum_test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead5317",
   "metadata": {},
   "source": [
    "## ILC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d043099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ILC_train = pd.read_csv(\"/kaggle/input/ilc-dataset/ILC_train_set.csv\")\n",
    "df_ILC_test = pd.read_csv(\"/kaggle/input/ilc-dataset/ILC_test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading civilSum dataset\n",
    "\n",
    "def load_data_civilSum(df, data):\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        judgement = row['text']\n",
    "        summary = row['summary']\n",
    "        data.append({\n",
    "            \"judgement\": judgement,\n",
    "            \"summary\": summary\n",
    "        })\n",
    "\n",
    "# loading ILC dataset\n",
    "\n",
    "def load_data_ILC(df, data):\n",
    "    for _, row in df.iterrows():\n",
    "        judgement = row['Case']\n",
    "        summary = row['Summary']\n",
    "        data.append({\n",
    "            \"judgement\": judgement,\n",
    "            \"summary\": summary\n",
    "        })\n",
    "\n",
    "# Loading IN-Abs dataset\n",
    "\n",
    "# judgement_folder = path to folder containing judgement text files\n",
    "# summary_folder = path to folder containing summary text files\n",
    "# max_files = max number of files to read\n",
    "\n",
    "def load_data(judgement_folder, summary_folder, data):\n",
    "    judgement_files = sorted(os.listdir(judgement_folder))\n",
    "    summary_files = sorted(os.listdir(summary_folder))\n",
    "\n",
    "    # Above judgement 001.txt corresponds to summary 001.txt\n",
    "    # Now lets loop through all those files in above 2 folders\n",
    "    for judgement_file, summary_file in zip(judgement_files, summary_files):\n",
    "        with open(os.path.join(judgement_folder, judgement_file), 'r') as f:\n",
    "            judgement = f.read()\n",
    "        with open(os.path.join(summary_folder, summary_file), 'r') as f:\n",
    "            summary = f.read()\n",
    "\n",
    "        # Now we create a {\"judgement\": judgement_text, \"summary\": summary_text} json representing each datapoint\n",
    "        data.append({\"judgement\": judgement, \"summary\": summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277fac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets load our dataset\n",
    "\n",
    "load_data_civilSum(df_civilSum_train, data_train)\n",
    "load_data_civilSum(df_civilSum_test, data_test)\n",
    "\n",
    "load_data_ILC(df_ILC_train, data_train)\n",
    "load_data_ILC(df_ILC_test, data_test)\n",
    "\n",
    "# Now lets load load the training and test data from our data set and then we store as a hugging face dataset\n",
    "# Huggingface dataset is a GPU-optimised, transformer-friendly version of a dataframe\n",
    "\n",
    "# Load training and test data\n",
    "load_data(\"/kaggle/input/legal-data-set/dataset/IN-Abs/train-data/judgement\", \"/kaggle/input/legal-data-set/dataset/IN-Abs/train-data/summary\", data_train)\n",
    "load_data(\"/kaggle/input/legal-data-set/dataset/IN-Abs/test-data/judgement\", \"/kaggle/input/legal-data-set/dataset/IN-Abs/test-data/summary\", data_test)\n",
    "\n",
    "train_data_in_abs = data_train.copy()\n",
    "test_data_in_abs = data_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data_in_abs))\n",
    "print(len(test_data_in_abs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a54fce",
   "metadata": {},
   "source": [
    "# Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa428d",
   "metadata": {},
   "source": [
    "We do not have a large enough dataset for extractive summarization so hence we get our own ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below we use to store our extractive datasets\n",
    "\n",
    "train_data_in_ext = []\n",
    "test_data_in_ext = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.data import find\n",
    "\n",
    "try:\n",
    "    find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad0f316",
   "metadata": {},
   "source": [
    "### Now lets generate our extractive datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f0780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We had already initialised our sentence transformer model\n",
    "# sentence_model = SentenceTransformer(\"Stern5497/sbert-legal-xlm-roberta-base\", device=\"cuda\") # We use gpu since it was available\n",
    "\n",
    "# cuda (gpu) is available, we had already checked earlier\n",
    "device = \"cuda\"\n",
    "\n",
    "# lets move the sentence_model to gpu\n",
    "sentence_model = sentence_model.to(device)\n",
    "\n",
    "# Maximum token limit for extractive summary output\n",
    "MAX_TOKENS = 1024\n",
    "\n",
    "# Function to split a judgement text into sentences using NLTK\n",
    "def split_into_sentences(judgement):\n",
    "    return nltk.sent_tokenize(judgement)\n",
    "\n",
    "# Function to get embeddings for each sentence (using GPU/CPU)\n",
    "def get_sentence_embeddings(sentences):\n",
    "    # Use the SentenceTransformer to get embeddings for each sentence\n",
    "    embeddings = sentence_model.encode(sentences, convert_to_tensor=True, show_progress_bar=False, device=device, batch_size=32)\n",
    "    return embeddings\n",
    "\n",
    "# Function to calculate similarity score between sentences and the abstractive summary (using GPU/CPU)\n",
    "def calculate_similarity_score(judgement_sentences, summary, sentence_embeddings):\n",
    "    # Move summary to device (GPU/CPU)\n",
    "    summary_embedding = sentence_model.encode([summary], convert_to_tensor=True, show_progress_bar=False, device=device, batch_size=32)\n",
    "    \n",
    "    # Calculate cosine similarity between the abstractive summary and each sentence\n",
    "    similarities = torch.nn.functional.cosine_similarity(sentence_embeddings, summary_embedding)\n",
    "    return similarities\n",
    "\n",
    "# Function to create extractive summarization based on similarity score\n",
    "def generate_extractive_summary(judgement, summary, max_tokens=MAX_TOKENS):\n",
    "    # Step 1: Split the judgement into sentences\n",
    "    sentences = split_into_sentences(judgement)\n",
    "    \n",
    "    # Step 2: Get embeddings for each sentence\n",
    "    sentence_embeddings = get_sentence_embeddings(sentences)\n",
    "    \n",
    "    # Step 3: Calculate similarity score between the abstractive summary and each sentence\n",
    "    similarities = calculate_similarity_score(sentences, summary, sentence_embeddings)\n",
    "    \n",
    "    # Step 4: Select sentences based on similarity score\n",
    "    selected_sentences = []\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for idx, score in enumerate(similarities):\n",
    "        sentence = sentences[idx]\n",
    "        sentence_tokens = len(sentence.split())  # Counting words as an approximation for token count\n",
    "        \n",
    "        # Stop if adding the sentence exceeds the token limit\n",
    "        if total_tokens + sentence_tokens <= max_tokens:\n",
    "            selected_sentences.append(sentence)\n",
    "            total_tokens += sentence_tokens\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Return the extractive summary as the selected sentences\n",
    "    extractive_summary = \" \".join(selected_sentences)\n",
    "    return extractive_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58119852",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SAVE_EVERY = 1000\n",
    "PARTIAL_SAVE_PATH = \"partial_train.json\"\n",
    "\n",
    "# loading existing data (resuming)\n",
    "train_data_in_ext = []\n",
    "start_idx = 0\n",
    "if os.path.exists(PARTIAL_SAVE_PATH):\n",
    "    with open(PARTIAL_SAVE_PATH, \"r\") as f:\n",
    "        train_data_in_ext = json.load(f)\n",
    "    start_idx = len(train_data_in_ext)\n",
    "    print(f\"Resuming from index {start_idx}\")\n",
    "\n",
    "\n",
    "for i in tqdm(range(start_idx, len(data_train))):\n",
    "    try:\n",
    "        entry = data_train[i]\n",
    "        judgement = entry[\"judgement\"]\n",
    "        summary = entry[\"summary\"]\n",
    "\n",
    "        extractive_summary = generate_extractive_summary(judgement, summary, max_tokens=MAX_TOKENS)\n",
    "\n",
    "        train_data_in_ext.append({\n",
    "            \"judgement\": judgement,\n",
    "            \"summary\": extractive_summary\n",
    "        })\n",
    "\n",
    "        # Periodic autosave\n",
    "        if (i + 1) % SAVE_EVERY == 0:\n",
    "            with open(PARTIAL_SAVE_PATH, \"w\") as f:\n",
    "                json.dump(train_data_in_ext, f)\n",
    "            print(f\"Autosaved at index {i + 1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed at index {i}: {str(e)}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635942fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Saving our final train extractive dataset, it took 8 hours for this to run and be created hence saving it\n",
    "with open(\"final_train_data.json\", \"w\") as f:\n",
    "    json.dump(train_data_in_ext, f)\n",
    "\n",
    "print(\"All done and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc496c5e",
   "metadata": {},
   "source": [
    "Similarly we do for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "BATCH_SIZE = 32\n",
    "SAVE_EVERY = 1000\n",
    "PARTIAL_SAVE_PATH = \"partial_test.json\"\n",
    "\n",
    "test_data_in_ext = []\n",
    "start_idx = 0\n",
    "if os.path.exists(PARTIAL_SAVE_PATH):\n",
    "    with open(PARTIAL_SAVE_PATH, \"r\") as f:\n",
    "        test_data_in_ext = json.load(f)\n",
    "    start_idx = len(test_data_in_ext)\n",
    "    print(f\"Resuming from index {start_idx}\")\n",
    "\n",
    "\n",
    "for i in tqdm(range(start_idx, len(data_test))):\n",
    "    try:\n",
    "        entry = data_test[i]\n",
    "        judgement = entry[\"judgement\"]\n",
    "        summary = entry[\"summary\"]\n",
    "\n",
    "        extractive_summary = generate_extractive_summary(judgement, summary, max_tokens=MAX_TOKENS)\n",
    "\n",
    "        test_data_in_ext.append({\n",
    "            \"judgement\": judgement,\n",
    "            \"summary\": extractive_summary\n",
    "        })\n",
    "\n",
    "        # Periodic autosave\n",
    "        if (i + 1) % SAVE_EVERY == 0:\n",
    "            with open(PARTIAL_SAVE_PATH, \"w\") as f:\n",
    "                json.dump(test_data_in_ext, f)\n",
    "            print(f\"Autosaved at index {i + 1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed at index {i}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# finally training our test extractive dataset\n",
    "with open(\"final_test_data.json\", \"w\") as f:\n",
    "    json.dump(test_data_in_ext, f)\n",
    "\n",
    "print(\"All done and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86757d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data_in_ext))\n",
    "print(len(train_data_in_abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ran this later for loading our saved ext data\n",
    "\n",
    "with open('/kaggle/input/final-train-data/final_train_data.json', 'r') as f:\n",
    "    train_data_in_ext = json.load(f)  \n",
    "\n",
    "with open('/kaggle/input/final-test-data/final_test_data.json', 'r') as f:\n",
    "    test_data_in_ext = json.load(f)  \n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_in_abs[18361])\n",
    "print(train_data_in_abs[6050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data_in_abs[18361]\n",
    "del train_data_in_abs[6050]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7d5e8",
   "metadata": {},
   "source": [
    "Now we have extractive summarization models of judgements, now we need to replace the original judgements by these summarization models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f19237",
   "metadata": {},
   "source": [
    "Now below we check whether there was any mismatch of data, if not then only we do the above replacement of abs dataset judgement by ext dataset summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# For train data\n",
    "\n",
    "mismatch_index_train = []\n",
    "mismatch_index_test = []\n",
    "\n",
    "successful = 0\n",
    "fail = 0\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(train_data_in_ext)), desc=\"Matching Training Judgements\"):\n",
    "    if train_data_in_ext[i][\"judgement\"] == train_data_in_abs[i][\"judgement\"]:\n",
    "        train_data_in_abs[i][\"judgement\"] = train_data_in_ext[i][\"summary\"]\n",
    "        successful += 1\n",
    "    else:\n",
    "        print(f\"Mismatch in judgement at index {i} in training data\")\n",
    "        fail += 1\n",
    "        mismatch_index_train.append(i)\n",
    "\n",
    "print(f\"\\nMatching complete: {successful} matched, {fail} mismatches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5341c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful = 0\n",
    "fail = 0\n",
    "\n",
    "# For testing data \n",
    "for i in tqdm(range(len(test_data_in_ext)), desc=\"Matching Testing Judgements\"):\n",
    "    if test_data_in_ext[i][\"judgement\"] == test_data_in_abs[i][\"judgement\"]:\n",
    "        test_data_in_abs[i][\"judgement\"] = test_data_in_ext[i][\"summary\"]\n",
    "        successful += 1\n",
    "    else:\n",
    "        print(f\"Mismatch in judgement at index {i} in test data\")\n",
    "        fail += 1\n",
    "        mismatch_index_test.append(i)\n",
    "\n",
    "print(f\"\\nMatching complete: {successful} matched, {fail} mismatches.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just random verifying\n",
    "\n",
    "print(train_data_in_abs[40][\"judgement\"])\n",
    "print()\n",
    "print(train_data_in_abs[40][\"summary\"])\n",
    "print()\n",
    "\n",
    "print(train_data_in_ext[40][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90fde65",
   "metadata": {},
   "source": [
    "# Training our Extractive Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6450fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "from tqdm.notebook import tqdm as notebook_tqdm\n",
    "from transformers import TrainerCallback\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Your pretrained sentence transformer\n",
    "# sentence_model = SentenceTransformer(\"Stern5497/sbert-legal-xlm-roberta-base\")\n",
    "\n",
    "# Classification model (LegalBERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", num_labels=2)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def create_sentence_classification_dataset(dataset):\n",
    "    data = []\n",
    "\n",
    "    # We wrapped the dataset with tqdm to show progress bar for the entire loop\n",
    "    for entry in tqdm(dataset, desc=\"Building sentence classification dataset\", disable=False):\n",
    "        judgement_sentences = sent_tokenize(entry[\"judgement\"])\n",
    "        summary_sentences = sent_tokenize(entry[\"summary\"])\n",
    "\n",
    "        if not judgement_sentences or not summary_sentences:\n",
    "            continue\n",
    "\n",
    "        summary_embeddings = sentence_model.encode(summary_sentences, convert_to_tensor=True, device=\"cuda\")\n",
    "        judgement_embeddings = sentence_model.encode(judgement_sentences, convert_to_tensor=True, device=\"cuda\")\n",
    "\n",
    "        cosine_sim_matrix = util.cos_sim(judgement_embeddings, summary_embeddings)\n",
    "\n",
    "        for i, sentence in enumerate(judgement_sentences):\n",
    "            similarity = cosine_sim_matrix[i].max().item()\n",
    "            label = 1 if similarity > 0.7 else 0\n",
    "            data.append({\"text\": sentence, \"label\": label})\n",
    "\n",
    "    return data\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)  # Each sentence is max 256 length\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This tokenize_fn function takes a single data example (which is a dictionary like {\"text\": sentence, \"label\": 0 or 1}) \n",
    "and tokenizes the sentence using your LegalBERT tokenizer so it can be fed into a transformer model.\n",
    "\"\"\"\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea188ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", num_labels=2)\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)  # Each sentence is max 256 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sentence-level labeled data\n",
    "sentence_data_train = create_sentence_classification_dataset(train_data_in_ext)\n",
    "sentence_data_test = create_sentence_classification_dataset(test_data_in_ext)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf93fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"sentence_data_train.json\", \"w\") as f:\n",
    "    json.dump(sentence_data_train, f, indent=2)\n",
    "\n",
    "with open(\"sentence_data_test.json\", \"w\") as f:\n",
    "    json.dump(sentence_data_test, f, indent=2)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec45d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/kaggle/input/sentence-tokenized-datasets/sentence_data_train.json', 'r') as f:\n",
    "    sentence_data_train = json.load(f)  \n",
    "\n",
    "with open('/kaggle/input/sentence-tokenized-datasets/sentence_data_test.json', 'r') as f:\n",
    "    sentence_data_test = json.load(f)  \n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b117a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to huggingFace dataset\n",
    "train_data_in_ext = Dataset.from_list(sentence_data_train).map(tokenize_fn, batched=True)\n",
    "test_data_in_ext = Dataset.from_list(sentence_data_test).map(tokenize_fn, batched=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data_in_ext)) \n",
    "print(len(test_data_in_ext))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190aef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, TrainerCallback\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d30ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bcf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./legalbert-extractive\",\n",
    "    # evaluation_strategy = \"epoch\", # We evaluate over validation set after each full epoch, this has nothing to do with validation set\n",
    "    learning_rate = 2e-5, # How fast the model learns\n",
    "    per_device_train_batch_size = 16, # We feed 16 samples at a time per GPU\n",
    "    per_device_eval_batch_size = 16,\n",
    "    weight_decay = 0.01, # Regularization to prevent overfitting\n",
    "    save_total_limit = 1, # Only the last 1 checkpoints will be saved to the disk\n",
    "    save_steps=1000, # we save checkpoints after 1000 steps\n",
    "    num_train_epochs=3, # number of epochs\n",
    "    # predict_with_generate = False, #  It makes sure the model uses .generate() internally for evaluation/prediction, so it can\n",
    "    # fp16=True, # Since GPU is available\n",
    "    logging_dir = \"./extractive/logs\", # Where it saves logs like loss, accuracy, evaluation scores.\n",
    "    logging_steps = 1000, # Prints training loss for every 100 steps\n",
    "    logging_strategy = \"steps\",\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "class TQDMCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        print(f\"Step {state.global_step} completed\")\n",
    "\n",
    "# Then in Trainer\n",
    "trainer_extractive = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data_in_ext,\n",
    "    eval_dataset=test_data_in_ext,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[TQDMCallback()]  \n",
    ")\n",
    "\n",
    "print(\"done check v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd11834",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Started\")\n",
    "trainer_extractive.train()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea48867",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./legalbert-extractive/final-model\")\n",
    "tokenizer.save_pretrained(\"./legalbert-extractive/final-model\")\n",
    "print(\"model saved succesfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b4c4d",
   "metadata": {},
   "source": [
    "# Training Abstractive Summ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a2e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_list(train_data_in_abs)\n",
    "test_dataset = Dataset.from_list(test_data_in_abs)\n",
    "\n",
    "# Hugging face is just a more smarter version of pandas dataframe and more optimized for NLP training\n",
    "# Hugging face Dataset can be thought as GPU-optimized, transformer-friendly version of a pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acb5924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our pretrained Pegasus Model:\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\").to(\"cuda\") # We let it use cuda since GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"judgement\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=256, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function_chunked(examples):\n",
    "    \n",
    "# Tokenizing our datasets\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "### Above input and output look like:\n",
    "\"\"\"\n",
    "Input:\n",
    "examples[\"judgement\"] = [\"The judge ruled in favor of the plaintiff.\"]\n",
    "examples[\"summary\"] = [\"Plaintiff wins the case.\"]\n",
    "\n",
    "Output:\n",
    "{\n",
    "    \"input_ids\": [[101, 2023, 2134, 2003, 2087, 1996, 1063, 1012, 102, 0, ...]], # Generated by input data\n",
    "    \"attention_mask\": [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...]], # Generated by input data\n",
    "    \"labels\": [[101, 2558, 2270, 1996, 2117, 102, 0, 0, ...]] # Generated by target data\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./legalbert-extractive\",\n",
    "    # evaluation_strategy = \"epoch\", # We evaluate over validation set after each full epoch, this has nothing to do with validation set\n",
    "    learning_rate = 2e-5, # How fast the model learns\n",
    "    per_device_train_batch_size = 16, # We feed 16 samples at a time per GPU\n",
    "    per_device_eval_batch_size = 16,\n",
    "    weight_decay = 0.01, # Regularization to prevent overfitting\n",
    "    save_total_limit = 1, # Only the last 1 checkpoints will be saved to the disk\n",
    "    save_steps=1000, # we save checkpoints after 1000 steps\n",
    "    num_train_epochs=3, # number of epochs\n",
    "    # predict_with_generate = False, #  It makes sure the model uses .generate() internally for evaluation/prediction, so it can\n",
    "    # fp16=True, # Since GPU is available\n",
    "    logging_dir = \"./extractive/logs\", # Where it saves logs like loss, accuracy, evaluation scores.\n",
    "    logging_steps = 1000, # Prints training loss for every 100 steps\n",
    "    logging_strategy = \"steps\",\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# Now lets create our data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer_abstractive = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_train_dataset,\n",
    "    eval_dataset = tokenized_test_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353891ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do our training\n",
    "trainer_abstractive.train() \n",
    "\n",
    "\n",
    "# Now lets save our model so that we can use it even after kernel restart\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./legal-pegasus-summarizer')\n",
    "tokenizer.save_pretrained('./legal-pegasus-summarizer')\n",
    "\n",
    "print(\"Abstractive model trained and saved successfully!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
